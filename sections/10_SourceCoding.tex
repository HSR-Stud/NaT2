\section{Informationstheorie und Quellencodierung \schaum{245-10}}
\textbf{Irrelevanz:} bedeutungslose Informationen. $\Rightarrow$ Muss daher nicht gesendet werden. \\
\textbf{Redundanz:} mehrfach vorhandene Information. \\
\textbf{Reduktion der Redundanz:} \parbox[t]{14cm}{Symbole mit hoher Auftretungswahrscheinlichkeit werden mit wenigen Bits, seltene Symbole mit mehr Bits codiert.}
\subsection{DMS - Discrete Memoryless Source}
Eine Informations\textbf{quelle} ist ein Objekt welches \textbf{Ereignise}, welche zufällig aus einer
WSK-Dichtefunktion ausgewählt werden, \textbf{generiert}. \\ 
Eine diskrete Quelle hat einen endlichen \textbf{Satz an Symbolen}, welcher auch \textbf{Alphabet}
genannt wird. Die Elemente dieses Satzes nennt man \textbf{Symbole} oder \textbf{Zeichen}. \\
Wird ein Symbol unabhängig vom Vorherigen generiert, so handelt es sich um eine DMS (diskrete
gedächtnisfreie Quelle). Eine solche wird mit folgenden Eigenschaften charakterisiert:
%\renewcommand{\parskip}{0}
\begin{itemize}\addtolength{\itemsep}{-0.3\baselineskip}
  \item Liste der Symbole - Alphabet
  \item Auftretenswahrscheinlichkeiten dieser Symbole - WSK-Dichtefunktion
  \item Symbolrate 
\end{itemize} 

\subsubsection{Informationsgehalt, Binary Unit, Entropie, Informationsrate \schaum{246-10.2-B.1,2,3}}
Mathematisch gesehen kann man sagen, je \textbf{unwahrscheinlicher} das \textbf{Eintreten} eines \textbf{spezifischen
Ereignisses} ist, desto \textbf{grösser} ist dessen \textbf{Informationsgehalt}.

\begin{multicols}{2}
\renewcommand{\arraystretch}{\arraystretchOriginal}
\begin{tabular}{|l|l|l|}
	\hline
	\textbf{Grösse}	& \textbf{Bezeichnung}	& \textbf{Einheit} \\ 
	\hline
	$I(x_i)$ 	& Informationsgehalt 	& b \\
	\hline
	$P(x_i)$	& Auftretens-WSK eines Symbols & \\
	\hline
	$H(x_i)$	& Entropie, mittlerer Informationsgehalt				& b/Symbol\\
	\hline
	$R(X)$		& Redundanz der Quelle			& b/Symbol \\
	\hline
	$R$			& Informationsrate 		& b/s \\
	\hline
	$r$			& Symbolrate/Bitrate	& Symbole/s	 \\
	\hline
\end{tabular}

Der Informationsgehalt kann in folgenden Masseinheiten angegeben werden:
\[ [I(X)] = \begin{cases}
            	\text{bit (\emph{bi}nary uni\emph{t})} 
            		& \text{falls } base=2. \\
            	\text{hartley oder decit}
            		& \text{falls } base=10. \\
            	\text{nat (\emph{na}tural uni\emph{t})} 
            		& \text{falls } base=e
			\end{cases} 
\]
\textbf{Meistens} wird $base=2$ verwendet, also bit oder gekürzt
\textbf{b}. Binary Unit ist ein Mass für den Informationsgehalt und sollte nicht mit dem Term ``bit'' (Binäres Zeichen) verwechselt
werden.
\end{multicols}
\begin{tabular}{l l l}


  	\textbf{Informationsgehalt:}
  	& $I(x_i) =  \log_{base} \frac{1}{P(x_i)} = - \log_{base} P(x_i)$
  	& \parbox[c]{10cm}{
  		$I(x_i) > I(x_k)$, falls $P(x_i) < 	P(x_k)$ \\
  		$I(x_i x_k) = I(x_i) + I(x_k)$, falls $x_i$ und $x_k$ unabh.
  	} \\
  
  	\textbf{Entropie:}
  	& $H(X)	= E[I(x_i)] = \sum\limits_{i=1}^m P(x_i) I(x_i) = - \sum\limits_{i=1}^m  P(x_i) \log_2{P(x_i)} $
  	& $ 0 \leq H(x) \leq \log_2(m)$, $m =$ Anzahl Symbole \\

	\textbf{Redundanz der Quelle:} 
	& $R(X) = H_{max}(X) - H(X) = \log_2(m) + \sum\limits_{i=1}^m  P(x_i) \log_2{P(x_i)}$ \\
	
	\textbf{Informationsrate:} 
	& $R = r \cdot H(X)$ \\
	
\end{tabular}

\subsection{DMC - Discrete Memoryless Channels \schaum{247-10.3}}
\begin{minipage}{9.5cm}
	\begin{center}
		\includegraphics[width=7.5cm]{../NaT2/bilder/10_DMC.png}
	\end{center}
\end{minipage}
\begin{minipage}{8.5cm}
	Ein DMC (diskreter gedächtnisfreier Kanal) ist ein statistisches Modell mit Eingang $X$ und
	Ausgang $Y$. \\ \\
	Er besitzt \textbf{$m$ Eingänge} und \textbf{$n$ Ausgänge}. \\ \\
	Alle Auftretenswahrscheinlichkeiten $P(x_i)$ der einzelnen Eingangs-Symbole werden als gegeben
	betrachtet. \\
	Jeder Übertragungspfad ist durch die Kanal-Übertragungs-Wahrscheinlichkeiten (channel transition
	probabilities) $P(y_j | x_i)$ definiert.
\end{minipage}

\subsubsection{Darstellung in Matritzenform}
\begin{minipage}[t]{9cm}
	\textbf{Kanalmatrix}\\
	Die Kanalmatrix gibt für jedes gesendete $x_i$ die bedingte Wahrscheinlichkeit $P(y_j|x_i)$ an, dass der Empfänger ein $y_j$ detektiert.
 	$$ \boxed{[P(Y | X)] = \begin{bmatrix}
              P(y_1 | x_1) & P(y_2 | x_1) & \ldots & P(y_n | x_1) \\
              P(y_1 | x_2) & P(y_2 | x_2) & \ldots & P(y_n | x_2) \\
             . & . & . & . \\
              P(y_1 | x_m) & P(y_2 | x_m) & \ldots & P(y_n | x_m)
           \end{bmatrix}}$$ \\
	$ [P(Y)] = [P(y_1) \quad P(y_2) \quad \ldots \quad P(y_n)] = [P(X)] \cdot [P(Y|X)]$ \\
%	$ [P(Y)] =  \\
	$ [P(X)] = [P(x_1) \quad P(x_2) \quad \ldots \quad P(x_m)]$ \\ \\
	$\sum\limits_{j=1}^n P(y_j | x_i) = 1 (\forall i)$ \qquad $\sum$ jeder Zeile von $[P(Y|X)]=1$\\ \\
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{9cm}
	\textbf{Verbundmatrix}\\
	Die Verbundmatrix gibt die Wahrscheinlichkeit von sämtlichen Kombinationen von gesendeten und empfangenen Symbolen $xi$, $y_j$ an.
	\begin{center}$ \boxed{[P(Y,X)] = \begin{bmatrix}
              P(y_1, x_1) & P(y_2, x_1) & \ldots & P(y_n, x_1) \\
              P(y_1, x_2) & P(y_2, x_2) & \ldots & P(y_n, x_2) \\
             . & . & . & . \\
              P(y_1, x_m) & P(y_2, x_m) & \ldots & P(y_n, x_m)
           \end{bmatrix}}$
	\end{center}
	$  [P(Y,X)] =  \begin{bmatrix}
    	P(x_1) & 0 & \ldots & 0 \\
    	0 & P(x_2) & \ldots & 0 \\
    	. & . & . & . \\
    	0 & 0 & \ldots & P(x_m)
    \end{bmatrix} [P(Y|X)] $\\ 
	Elemente auf der Diagonale sollten den grössten Wert gegenüber anderen Elementen auf der Zeile
	besitzen. \\
	$\sum $ aller Elemente von $[P(Y,X)] = 1$
\end{minipage}

\subsubsection{Spezielle Kanäle \schaum{248-10.3.C}}
\textbf{Verlustfreier (lossless) Kanal} \\
\begin{minipage}{14cm}
	Auf jeder Spalte der Kanalmatrix gibt es jeweils nur ein  Element $\neq 0$. \\

	$$ [P(Y | X)] = \begin{bmatrix}
              \frac34 & \frac14 & 0 & 0 & 0 \\
              0 & 0 & \frac13 & \frac23 & 0 \\
              0 & 0 & 0 & 0 & 1
           \end{bmatrix}$$ \\
\end{minipage}
\begin{minipage}{5cm}
\begin{center}
	\adjustbox{width=5cm}{\input{./tikz/channel_lossless}}
\end{center}
\end{minipage}

\textbf{Deterministischer (deterministic) Kanal} \\
\begin{minipage}{14cm}
	Auf jeder Zeile der Kanalmatrix gibt es jeweils nur ein  Element $\neq 0$, welches $1$ sein
	muss.

	$$ [P(Y | X)] = \begin{bmatrix}
           		1 & 0 & 0 \\
           		1 & 0 & 0 \\
           		0 & 1 & 0 \\
           		0 & 1 & 0 \\
           		0 & 0 & 1
           \end{bmatrix}$$
\end{minipage}
\begin{minipage}{5cm}
\begin{center}
	\adjustbox{width=5cm}{\input{./tikz/channel_deterministic}}
\end{center}
\end{minipage}

\textbf{Rauschfreier (noiseless) Kanal} \\
\begin{minipage}{14cm}
	Die Kanalmatrix entspricht der Einheitsmatrix.

	$$ [P(Y | X)] = \begin{bmatrix}
           		1 & 0 & \ldots & 0\\
           		0 & 1 & \ldots & 0\\
           		\vdots & \vdots & \vdots & \vdots \\
           		0 & 0 & \ldots & 1\\
           \end{bmatrix} = I$$ \\
\end{minipage}
\begin{minipage}{4cm}
\begin{center}
	\adjustbox{width=5cm}{\input{./tikz/channel_noiseless}}
\end{center}
\end{minipage}

\textbf{Binärer (binary) Kanal} \\
\begin{minipage}{14cm}
	$$ [P(Y | X)] = \begin{bmatrix}
           		1-p_1 & p_1 \\
           		p_2 & 1-p_2
           \end{bmatrix} $$
    $$ p = p_e = Fehler-WSK$$
    $$ \text{symmetrisch: } p_1 = p_2$$
\end{minipage}
\begin{minipage}{4cm}
\begin{center}
	\adjustbox{width=5cm}{\input{./tikz/channel_binary}}
\end{center}
\end{minipage}

\subsubsection{Bedingte Entropien , Gegenseitige Information \schaum{250-10.4}}
	\textbf{Bedingte Entropie $H(Y|X)$}\\
	$\bullet$ Informationsbedarf um aus gesendetem X empfangenes Y zu bestimmen: $H(Y|X)	=- \sum\limits_{j=1}^{n} \sum\limits_{i=1}^{m} P(x_i,y_j) log_2(P(y_j|x_i))$ \\
	\textbf{Bedingte Entropie $H(X|Y)$}\\
	$\bullet$ Informationsbedarf um aus empfangenes Y gesendetem X zu bestimmen: $H(X|Y)	=- \sum\limits_{j=1}^{n} \sum\limits_{i=1}^{m} P(x_i,y_j) log_2(P(x_i|y_j))$ \\
	\textbf{Verbund-Entropie $H(X,Y)$} \\
	$\bullet$ Information des gesamten Kommunikationskanal:  $H(X,Y)	=- \sum\limits_{j=1}^{n} \sum\limits_{i=1}^{m} P(x_i,y_j) log_2(P(x_i,y_j))$ \qquad \parbox{6cm}{
		$H(X,Y) = H(X|Y) + H(Y)$ \\
		$H(X,Y) = H(Y|X) + H(X)$
	} \\
	\textbf{Gegenseitige (mutual) Information} in b/(Symbol)\\
	$\bullet$ Information, welche vom Eingang zum Ausgang des Kanales transferiert wird: $I(X;Y) = H(X)-H(X|Y)$\\
	\hspace*{0.5cm} Eigenschaften: \quad \parbox{12cm}{
	$I(X;Y) = H(X)-H(X|Y) = H(Y)-H(Y|X) = I(Y;X)$ \\
	$I(X;Y) = H(X)+H(Y)-H(X,Y)$}

\subsection{Kanalkapazität \schaum{251-10.5}}
\begin{minipage}[c]{8cm}
	$$ C_s = \max\limits_{\{ P(x_i) \}}{I (X; Y)} $$
	$$ C = r_b C_s $$
	$$ r_s H(X) \leq C$$
\end{minipage}
\begin{minipage}[c]{10cm}
	$C_s$ Kanalkapazität pro Symbol, $[C_s]$ = b/Symbol \\
	$C$ Kanalkapazität pro Sekunde, $[C]$ = b/s \\
	$r_s$ Symbolrate Quelle, $r_b$ Bitrate Kanal, $[r]$ = Symbol/s \\
	Bedingung für eine (theoretisch) fehlerfreie Übertragung
\end{minipage}

\subsubsection{Kanalkapazitätien spezieller Kanäle}

	\renewcommand{\arraystretch}{2}
	\begin{tabular}{| p{3.5cm} | p{7.5cm} | p{6.5cm} |}
		\hline  
    		\textbf{Verlustfrei}
    			& $ I(X; Y) = H(X) $
    			& $ C_s = \max\limits_{\{ P(x_i) \}}{H (X)} = \log_2 m $ \\
		\hline
    		\textbf{Deterministisch}
    			& $ I(X; Y) = H(Y) $
    			& $ C_s = \max\limits_{\{ P(x_i) \}}{H (Y)} = \log_2 n $ \\
		\hline
    		\textbf{Rauschfrei}
    			& $ I(X; Y) = H(Y) = H(X)$
    			& $ C_s = \log_2 n = \log_2 m$ \\
		\hline
    		\textbf{Binär Symmetrisch}
    			& $ I(X; Y) = H(Y) + p \log_2 p + (1-p) \log_2 (1-p)$
    			& $ C_s = 1 + p \log_2 p + (1-p) \log_2 (1-p)$ \\
		\hline
    		\textbf{AWGN-Kanal}
    			& $ C = 2 B C_s = B \log_2 (1 + \left(\frac{S}{N}\right)_0)$ 
    			& $ C_s = \max{I(X; Y) = \frac{1}{2} \log_2 (1 + \left(\frac{S}{N}\right)_0)}$ \\
		\hline
 	\end{tabular}
	\renewcommand{\arraystretch}{1} \\ \\
Wobei $B$ der Bandbreite des Kanals entspricht; $ m =$ Anzahl Symbole von $X$ ; $n=$ Anzahl Symbole von $Y$
% \subsection{AWGN-Channel \schaum{252-10.6}}
% \textbf{TODO}

\subsection{Quellencodierung \schaum{253-10.7}}
 Der Quellencoder wandelt das Nachrichtensignal einer diskreten Quelle in eine Symbolfolge mit \textbf{möglichst kleiner Redundanz} um.
\subsubsection{Code-Länge, -Effizienz, -Redundanz \schaum{253-10.7.A/B}}
Gilt für eine DMS mit endlicher Entropie.
\begin{multicols}{2}
	\abovedisplayskip=-15pt % Hack um den überflüssigen Whitespace zu entfernen.
	\begin{align*}
		L 		 &= \sum\limits_{i=1}^m P(x_i) n_i \geq H(X)\\
		\eta	 & = \frac{L_{min}}{L} = \frac{H(X)}{L}  \\
		R_c(X) &  = L - H(X) \\
		\gamma_c &  = 1-\eta \\
		R_q(X)	 &= \gamma_q = H_{max} - H(x) = \log_2(m) - H(X) \\
	\end{align*}
	
	\begin{tabular}{lll}
		$L$ 		& Mittlere Codelänge & $[L]$ = b/Symbol \\
		$L_{min}$ 	& kleinstmögliche mittlere Codelänge &  \\
		$P(x_i)$	& Auftretens-WSK des Symbols \\
		$n_i$ 		& Symbollänge & $[n_i] = $ Bits \\
		$m$ 		& Anzahl Symbole des Codes \\
		$\eta$ 		& Effizienz \\
		$\gamma_c$  & Code-Redundanz  & \\
		$R_c(X)$    & Redundanz der Codierung & $[R_c(X)] = $ b/Symbol\\
		$H(X)$ 		& Entropie &  $[H(X)] = $ b/Symbol 
	\end{tabular}
\end{multicols}

\subsubsection{Klassifizierung von Codes \schaum{254-10.7.C}}
	\renewcommand{\arraystretch}{2}
	\begin{tabular}{| p{6cm} | p{12cm} |}
		\hline
    	\textbf{Bezeichnung} & \textbf{Eigenschaften}  \\
		\hline  
    	\begin{minipage}[c]{6cm}  
	    	\textbf{Fixed Lengh Code} \\
	    	feste Länge 
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}    
	    		Alle Codewörter haben die gleiche Länge. \\
	    		Bsp.: ASCII-Code. 
      	\end{minipage}
    	\\
		\hline
    	\begin{minipage}[c]{6cm}    
    		\textbf{Variable Lengh Code} \\
    		variable Länge 
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}    
    		Codewörter haben unterschiedliche Länge. \\
    		Bsp.: Shannon-Fano, Huffman und Morse Code. 
      	\end{minipage}
    	\\
		\hline
    	\begin{minipage}[c]{6cm}    
    		\textbf{Prefix-Free Code} \\
    		präfixfrei 
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}    
    		Kein Codewort dient als Präfix (Vorsible) für ein anderes Codewort. \\
    		Bsp.: Shannon-Fano oder Huffman Code, aber nicht Morse-Code. 
      	\end{minipage}
    	\\
		\hline
    	\begin{minipage}[c]{6cm}    
    		\textbf{Uniquely Decodeable Code} \\
    		eindeutig decodierbar 
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}    
    		Kette von Codewörtern kann eindeutig wieder in die ursrünglichen Symbolfolgen
    		zurückgewandelt werden. \\
    		\textbf{Jeder präfixfreie Codes ist eindeutig decodierbar}. 
      	\end{minipage}
    	\\
		\hline
    	\begin{minipage}[c]{6cm}    
    		\textbf{Instantaneous Code} \\
    		sofort decodierbar 
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}    
    		Liefert nach Empfang jedes einzelnen Codeworts sofort ein eindeutiges Symbol. \\
    		Jeder prefixfreie Code ist ein Instantaneous Code.
      	\end{minipage}
    	\\
		\hline
    	\begin{minipage}[c]{6cm}    
    		\textbf{Optimal Code}  
      	\end{minipage}
    	& \begin{minipage}[c]{12cm}   
    		Jeder Instantaneous Code mit minimaler Codelänge ist ein optimaler Code. \\ 
    		$\eta = 1 = 100 \% $ 
      	\end{minipage}
    	\\
		
		\hline
 	\end{tabular}
	\renewcommand{\arraystretch}{1}

\subsubsection{Kraft'sche Ungleichung \schaum{255-10.7.D}}
Wenn diese Ungleichung erfüllt ist, besagt sie, dass ein \textbf{eindeutig} und \textbf{sofort
decodierbarer} Code gefunden werden kann. \\
Gegeben ist eine Quelle mit \textbf{Alphabet $x_i$} der \textbf{Länge $m$}, wobei jedem
\textbf{Symbol $x_i$} kein Codewort aber eine \textbf{Codelänge $n_i$} zugewiesen ist.
$$ K = \sum\limits_{i=1}^{m} 2 ^{-n_i} \leq 1$$ 
Die Kraft'sche Ungleichung hilft jedoch zum Auffinden dieses Codes nicht weiter.

\subsection{Entropie Codierung \schaum{255-10.8}}
\textbf{Ziel:} Durchschnittliche Codelänge an Entropie annähern $ \Rightarrow $ Wirkungsgrad
steigern.

\subsubsection{Shannon - Fano Codierung \schaum{255-10.8.A}}
\begin{multicols}{2}
	\begin{tabular}{|c|c|c|c|c|c|l|}
		\hline
		$x_i$	& $P(x_i)$ 	& Step 1	& Step 2	& Step 3 	& Step 4 	& Code \\
		\hline
		$x_1$	& 0.30		& 0			& 0			& 			&			& 00 \\
		\cline{4-4}
		$x_2$	& 0.25		& 0			& 1			& 			& 			& 01 \\
		\cline{3-4}
		$x_3$	& 0.20		& 1			& 0			& 			& 			& 10 \\
		\cline{4-5}
		$x_4$	& 0.12		& 1			& 1			& 0			& 			& 110 \\
		\cline{5-6}
		$x_5$	& 0.08		& 1			& 1			& 1			& 0			& 1110 \\
		\cline{6-6}		
		$x_6$	& 0.05		& 1			& 1			& 1			& 1			& 1111 \\
		\hline 
	\end{tabular}
	
	\columnbreak
	
	\textbf{Vorgehen:}
	\begin{enumerate}
		\item Symbole nach \textbf{absteigender} Wahrscheinlichkeit anordnen.
		\item Trennung in \textbf{2 Teilmengen} mit möglichst gleicher WSK.
		\item \textbf{Obere} Teilmenge Symbol \textbf{0}, \textbf{unterer} Teilmenge Symbol \textbf{1}
		  zuordnen.
		\item Weiterhin unterteilen gemäss obigen Schritten, bis keine Teilung mehr möglich ist.
	\end{enumerate}
	
	\textbf{Eigenschaften:}
	\begin{itemize}
  		\item Präfix frei
  		\item letzter Teilungsschrit liefert immer gerade Anzahl Codewörter 
	\end{itemize}
\end{multicols}



\subsubsection{Huffman Codierung \schaum{256-10.8.B}}
\begin{multicols}{2}
  \adjustbox{width=0.95\linewidth, frame}{\input{./tikz/10_codierung_huffman}}
  
  \columnbreak
  
  	\textbf{Vorgehen:}
  	\begin{enumerate}
	  \item Symbole nach \textbf{absteigender} Wahrscheinlichkeit anordnen.
	  \item \textbf{Unterste 2} Symbole als Gruppe zusammenfassen.
	  \item Beide Schritte wiederholen, bis nur noch zwei Gruppen vorliegen.
	  \item \textbf{Grössere} WSK \textbf{0}, \textbf{kleinere} WSK \textbf{1} zuordnen.
	  \item Reduktion rückgägngig machen und vorheriger Schritt für alle Teilschritte wiederholen.
	\end{enumerate}	
	
	\textbf{Eigenschaften:}
	\begin{itemize}
	  \item Maximale Effizienz
	  \item Keine reduntante Bits (vgl. Prüfung Aug. 2010)
	\end{itemize}
\end{multicols}
